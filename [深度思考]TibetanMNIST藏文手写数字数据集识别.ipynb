{"cells":[{"outputs":[],"execution_count":null,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"B8EBC0FFF4784464826E0334248D988E","scrolled":false}},{"outputs":[],"execution_count":null,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"59C179028570483E89F4A97CF65C9505"}},{"metadata":{"id":"E03501409EA2459382CE75BBEA13772A","collapsed":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"step 0, loss 5.960211 test accuracy 0.145000\nstep 100, loss 0.674428 test accuracy 0.885000\nstep 200, loss 0.572597 test accuracy 0.910000\nstep 300, loss 0.278655 test accuracy 0.925000\nstep 400, loss 0.334951 test accuracy 0.945000\nstep 500, loss 0.182906 test accuracy 0.960000\nstep 600, loss 0.131640 test accuracy 0.955000\nstep 700, loss 0.102483 test accuracy 0.970000\nstep 800, loss 0.162252 test accuracy 0.965000\nstep 900, loss 0.103369 test accuracy 0.965000\nstep 1000, loss 0.099146 test accuracy 0.975000\nstep 1100, loss 0.102525 test accuracy 0.975000\nstep 1200, loss 0.138506 test accuracy 0.965000\nstep 1300, loss 0.070248 test accuracy 0.970000\nstep 1400, loss 0.042735 test accuracy 0.970000\nstep 1500, loss 0.071444 test accuracy 0.980000\nstep 1600, loss 0.066517 test accuracy 0.980000\nstep 1700, loss 0.053191 test accuracy 0.970000\nstep 1800, loss 0.092048 test accuracy 0.975000\nstep 1900, loss 0.065970 test accuracy 0.975000\nstep 2000, loss 0.031999 test accuracy 0.980000\nstep 2100, loss 0.031096 test accuracy 0.980000\nstep 2200, loss 0.035539 test accuracy 0.980000\nstep 2300, loss 0.058202 test accuracy 0.975000\nstep 2400, loss 0.081459 test accuracy 0.980000\nstep 2500, loss 0.031238 test accuracy 0.980000\nstep 2600, loss 0.008019 test accuracy 0.990000\nstep 2700, loss 0.006568 test accuracy 0.980000\nstep 2800, loss 0.039677 test accuracy 0.980000\nstep 2900, loss 0.009853 test accuracy 0.985000\nstep 3000, loss 0.011483 test accuracy 0.985000\nstep 3100, loss 0.059218 test accuracy 0.985000\nstep 3200, loss 0.029296 test accuracy 0.985000\nstep 3300, loss 0.020868 test accuracy 0.985000\nstep 3400, loss 0.033389 test accuracy 0.980000\nstep 3500, loss 0.006509 test accuracy 0.985000\nstep 3600, loss 0.025261 test accuracy 0.985000\nstep 3700, loss 0.013075 test accuracy 0.980000\nstep 3800, loss 0.033118 test accuracy 0.985000\nstep 3900, loss 0.012591 test accuracy 0.985000\nstep 4000, loss 0.040675 test accuracy 0.990000\nstep 4100, loss 0.008919 test accuracy 0.985000\nstep 4200, loss 0.014542 test accuracy 0.980000\nstep 4300, loss 0.019433 test accuracy 0.985000\nstep 4400, loss 0.006040 test accuracy 0.985000\nstep 4500, loss 0.007214 test accuracy 0.990000\nModel saved in file: /home/kesci/input/TibetanMNIST5610/ckpt1/my_model.ckpt-4500\nfinal test accuracy 0.988745\n","name":"stdout"}],"source":"\n# coding: utf-8\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n# In[1]:\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import utils\nimport pandas as pd\nimport matplotlib.pyplot as plt\nexcl = lambda x:os.popen(x).readlines() \n\n\n# In[3]:\n\ndata_path = \"/home/kesci/input/TibetanMNIST5610/TibetanMNIST.npz\"\ndata = np.load(data_path)\n\nx_data = data['image'].reshape(17768, 784)\ny_data = utils.to_categorical(data['label'], 10)\n\ndatas = []\n\nfor (x,y) in zip(x_data,y_data):    \n    datas.append({\"x\":x/255,\"y\":y})\n    \ndatas = np.array(datas)\nnp.random.shuffle(datas)\nseq = 0.8\nlens = len(datas)\ntrains = datas[0:int(seq*lens)]\ntests = datas[int(seq*lens):]\nx_train = []\ny_train = []\nx_test = []\ny_test = []\n\nfor t in trains:\n    x_train.append(t[\"x\"])\n    y_train.append(t[\"y\"])\nfor t in tests:\n    x_test.append(t[\"x\"])\n    y_test.append(t[\"y\"])\n\nx_train = np.array(x_train).astype(np.float32)\ny_train = np.array(y_train).astype(np.float32)\nx_test = np.array(x_test).astype(np.float32)\ny_test = np.array(y_test).astype(np.float32)\nlens_train,_=x_train.shape\nlens_test,_=x_test.shape\nlens_train,lens_test,x_train.shape,y_train.shape\n\n\n# In[4]:\n\n#训练数据  \nx = tf.placeholder(\"float\", shape=[None, 784],name=\"x\")  \n#训练标签数据  \ny_ = tf.placeholder(\"float\", shape=[None, 10],name=\"y_\")  \n#把x更改为4维张量，第1维代表样本数量，第2维和第3维代表图像长宽， 第4维代表图像通道数, 1表示灰度  \nx_image = tf.reshape(x, [-1,28,28,1])  \n  \n#第一层：卷积层  \nconv1_weights = tf.get_variable(\"conv1_weights\", [5, 5, 1, 32], initializer=tf.truncated_normal_initializer(stddev=0.1)) #过滤器大小为5*5, 当前层深度为1， 过滤器的深度为32  \nconv1_biases = tf.get_variable(\"conv1_biases\", [32], initializer=tf.constant_initializer(0.0))  \nconv1 = tf.nn.conv2d(x_image, conv1_weights, strides=[1, 1, 1, 1], padding='SAME') #移动步长为1, 使用全0填充  \nrelu1 = tf.nn.relu( tf.nn.bias_add(conv1, conv1_biases) ) #激活函数Relu去线性化  \n\n#第二层：最大池化层  \n#池化层过滤器的大小为2*2, 移动步长为2，使用全0填充  \npool1 = tf.nn.max_pool(relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  \n\n#第三层：卷积层  \nconv2_weights = tf.get_variable(\"conv2_weights\", [5, 5, 32, 64], initializer=tf.truncated_normal_initializer(stddev=0.1)) #过滤器大小为5*5, 当前层深度为32， 过滤器的深度为64  \nconv2_biases = tf.get_variable(\"conv2_biases\", [64], initializer=tf.constant_initializer(0.0))  \nconv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME') #移动步长为1, 使用全0填充  \nrelu2 = tf.nn.relu( tf.nn.bias_add(conv2, conv2_biases) )  \n\n#第四层：最大池化层  \n#池化层过滤器的大小为2*2, 移动步长为2，使用全0填充  \npool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  \n  \n#第五层：全连接层  \nfc1_weights = tf.get_variable(\"fc1_weights\", [7 * 7 * 64, 1024], initializer=tf.truncated_normal_initializer(stddev=0.1)) #7*7*64=3136把前一层的输出变成特征向量  \nfc1_baises = tf.get_variable(\"fc1_baises\", [1024], initializer=tf.constant_initializer(0.1))  \npool2_vector = tf.reshape(pool2, [-1, 7 * 7 * 64])  \nfc1 = tf.nn.relu(tf.matmul(pool2_vector, fc1_weights) + fc1_baises)  \n  \n#为了减少过拟合，加入Dropout层  \nkeep_prob = tf.placeholder(tf.float32,name=\"keep_prob\") \nfc1_dropout = tf.nn.dropout(fc1, keep_prob)  \n\n#第六层：全连接层  \nfc2_weights = tf.get_variable(\"fc2_weights\", [1024, 10], initializer=tf.truncated_normal_initializer(stddev=0.1)) #神经元节点数1024, 分类节点10  \nfc2_biases = tf.get_variable(\"fc2_biases\", [10], initializer=tf.constant_initializer(0.1))  \nfc2 = tf.matmul(fc1_dropout, fc2_weights) + fc2_biases  \n  \n#第七层：输出层  \n# softmax  \ny_conv = tf.nn.softmax(fc2,name=\"y_conv\")  \ny_conv_labels = tf.argmax(y_conv,1,name='y_conv_labels')\n#定义交叉熵损失函数  \ny_conv = tf.clip_by_value(y_conv,1e-4,1.99)\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))  \n  \n#选择优化器，并让优化器最小化损失函数/收敛, 反向传播  \ntrain_step = tf.train.AdamOptimizer(0.00008).minimize(cross_entropy)  \n  \n# tf.argmax()返回的是某一维度上其数据最大所在的索引值，在这里即代表预测值和真实值  \n# 判断预测值y和真实值y_中最大数的索引是否一致，y的值为1-10概率  \ncorrect_prediction = tf.equal(y_conv_labels, tf.argmax(y_,1))  \n  \n# 用平均值来统计测试准确率  \naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name=\"accuracy\") \n\n\n# In[1]:\n\nwith tf.Session() as sess:\n\n    #开始训练  \n    srun = sess.run\n    srun(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n    \n    for i in range(4501):  \n        start_step = i*100 % lens_train\n        stop_step = start_step+100\n\n        batch_x, batch_y = x_train[start_step:stop_step,:], y_train[start_step:stop_step,:]\n        cross_entropy_val,_=srun([cross_entropy,train_step],feed_dict={x: batch_x, y_: batch_y, keep_prob: 0.5}) #训练阶段使用50%的Dropout  \n        if i%100 == 0:  \n            train_accuracy = srun(accuracy,feed_dict={x:x_test[:200], y_:y_test[:200], keep_prob: 1.0}) #评估阶段不使用Dropout  \n            print(\"step %d, loss %f test accuracy %f\" % (i, cross_entropy_val, train_accuracy))  \n            saver_path = saver.save(sess, \"/home/kesci/input/TibetanMNIST5610/ckpt1/my_model.ckpt\",global_step=i)  # 将模型保存到save/model.ckpt文件\n\n    \n\n    print(\"Model saved in file:\", saver_path)\n    \n    #在测试数据上测试准确率  \n    print(\"final test accuracy %g\" % srun(accuracy,feed_dict={x: x_test, y_: y_test, keep_prob: 1.0}))  \n    \n\n\n# In[ ]:\n\n\n\n","execution_count":15}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"version":"3.5.5","name":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}